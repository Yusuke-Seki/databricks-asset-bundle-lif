{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession as SparkSession, functions as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINE_SEP = \"  \\n\"\n",
    "SLACK_URL = \"https://www.youtube.com/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tags_json_to_rows(json_str: str) -> list[tuple[str, str]]:\n",
    "    \"\"\"'{\"k\":\"v\",\"a\":\"b\"}' -> [(\"k\",\"v\"),(\"a\",\"b\")]。Noneや壊れたJSONは空配列を返す。\"\"\"\n",
    "    if not json_str:\n",
    "        return []\n",
    "    try:\n",
    "        obj = json.loads(json_str)\n",
    "        if isinstance(obj, dict):\n",
    "            # すべて文字列化（数値/真偽も文字列にする運用ならここでstr()）\n",
    "            return [(str(k), \"\" if v is None else str(v)) for k, v in obj.items()]\n",
    "        else:\n",
    "            return []\n",
    "    except Exception:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 引数の受け取り\n",
    "    dbutils.widgets.text(\"SOURCE_TABLE\", \"\")\n",
    "    source_table = dbutils.widgets.get(\"SOURCE_TABLE\")\n",
    "    \n",
    "    stage = os.environ.get(\"STAGE\") \n",
    "    where_clause = \"WHERE catalog RLIKE '_dev$'\" if (stage != \"prod\") else \"\"\n",
    "\n",
    "    # SQL文の生成\n",
    "    sql_text = f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {source_table} \n",
    "    {where_clause}\n",
    "    \"\"\"\n",
    "    print(sql_text)\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    df_target = spark.sql(sql_text)\n",
    "\n",
    "    display(df_target) \n",
    "except Exception as e:\n",
    "    raise Exception(f\"`{source_table}`テーブルの取得に失敗しました。SQL: \\\"{sql_text}\\\", Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_target.count() == 0:\n",
    "    print(\"Descriptionの更新対象がなかったため、処理をスキップして終了しました。\")\n",
    "else:\n",
    "    error_records = []\n",
    "    df_target_sorted = df_target.orderBy(F.col(\"catalog\"), F.col(\"schema\"), F.col(\"table\"))\n",
    "    for row in df_target_sorted.toLocalIterator():\n",
    "        # カラムの取得\n",
    "        catalog         = row['catalog']\n",
    "        schema          = row['schema']\n",
    "        table           = row['table']\n",
    "        table_category  = row['table_category']\n",
    "        tag_json        = row['tag_json']\n",
    "\n",
    "        kvs = parse_tags_json_to_rows(tag_json)\n",
    "        tags_text = \"\"\n",
    "        for k, v in kvs:\n",
    "            if tags_text != \"\":\n",
    "                tags_text += \",\" \n",
    "            tags_text += f\"'{k}' = '{v}'\"\n",
    "\n",
    "        # SQLの実行\n",
    "        sql = \"\"\n",
    "        try:\n",
    "            if table_category == 0: # Schemaの場合\n",
    "                sql= f\"ALTER SCHEMA {catalog}.{schema} SET TAGS ({tags_text});\"\n",
    "            elif table_category == 1: # Tableの場合\n",
    "                sql = f\"ALTER TABLE {catalog}.{schema}.{table} SET TAGS ({tags_text});\"\n",
    "            elif table_category == 2: # Materialized Viewの場合\n",
    "                sql = f\"ALTER MATERIALIZED VIEW {catalog}.{schema}.{table} SET TAGS ({tags_text});\"\n",
    "            elif table_category == 3: # Viewの場合\n",
    "                sql = f\"ALTER VIEW {catalog}.{schema}.{table} SET TAGS ({tags_text});\"\n",
    "            else:\n",
    "                raise ValueError(f\"table_category is not 1 or 2: table_category={table_category}\")\n",
    "            spark.sql(sql)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_records.append({\n",
    "                \"catalog\": catalog,\n",
    "                \"schema\": schema,\n",
    "                \"table\": table,\n",
    "                \"table_category\": table_category,\n",
    "                \"error_category\": \"update_tag_error\",\n",
    "                \"tag_json\": tag_json,\n",
    "                \"sql\": sql,\n",
    "                \"error_message\": str(e)\n",
    "            })\n",
    "\n",
    "    # エラーがある場合、エラーを表示\n",
    "    if len(error_records) > 0:\n",
    "        df_error = pd.DataFrame(error_records).astype(\"string\")\n",
    "        display(df_error)\n",
    "        raise Exception(\"Tagsの更新に失敗したレコードがあります。\")\n",
    "    else:\n",
    "        print(\"Tagsの更新がすべて成功しました。\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
